# esl_queuelock: a user-space queueing lock intended for high-contention regions

Esl_queuelock implements a queueing lock in user space that should perform better than pthreads locks when contention for the locked region is high (i.e., there is a significant chance that the lock is locked when a thread tries to acquire it).  Pthreads provides two ways to acquire a lock: `pthreads_mutex_lock()`, which blocks until it acquires the lock, and `pthreads_mutex_trylock()`, which attempts to acquire the lock and returns immediately with a result that indicates whether it succeeded or not.  These functions are typically extremely fast (~25ns, according to some experiments) when acquiring a lock that is not currently held, as they can be implemented using only a few instructions, including a read-modify-write operation on the location that controls whether the lock is held.  However, they perform much less well when acquiring a lock that is already held, particularly when many threads are trying to acquire the lock.

When a thread uses `pthread_mutex_lock()` to acquire a lock that is already held, Pthreads makes a kernel call to put the thread on a list of waiting threads and signals the OS scheduler to de-schedule the now-waiting thread. (Note: this is how Pthreads typically works, but different implementations may vary.)  This is very useful in environments where there are more threads than execution resources (cores), as it allows another thread, which may be the one holding the lock, to begin executing, but is less necessary in a many-core environment where the number of threads in an application is typically matched to the number of threads the hardware can execute simultaneously.  Alternately, programs can use `pthreads_mutex_trylock()` to implement a spin-lock by looping until they acquire the lock.  This avoids the overhead of a kernel call, but still performs less well as the number of threads waiting on the lock increases due to contention for the memory location that contains the lock.

Esl_queuelock moves the list of threads waiting on the lock from kernel to user space to improve performance.  When a lock is not currently held, it can be acquired by acquiring the pthreads lock that controls access to the esl_queuelock object and setting the esl_queuelock object's *locked* field to 1, which takes only a few more instructions than acquiring an un-held pthreads lock.  To acquire a currently-held lock, a thread locks the pthreads lock on the esl_queuelock object and then adds itself to the waiting list for the lock by identifying the next available location in the lock's *waiting* field, ensuring that that location is set to zero, and then spinning on that location until it is set to one.  The key difference between this approach and spin-waiting using `pthreads_mutex_trylock()` is that each waiting thread spins on a different location and those locations are spaced 64 bytes apart so that they will lie on different cache lines in most current systems.  This eliminates the cache contention that degrades performance when spin-waiting on a single location, particularly when spin-waiting is implemented using an atomic read-modify-write operation.

To release an esl_queuelock(), a thread acquires the lock on the data structure and determines if any threads are waiting for the lock.  If so, it writes a one into the wait location of the next thread in the queue, updates the structure to record that there is one fewer waiting thread, unlocks the structure, and returns.  If there are no threads waiting for the lock, the releasing thread simply sets the lock's *locked* field to 0, unlocks the pthreads lock, and returns.

While esl_queuelock still relies on pthreads locks, it is only necessary to acquire the pthreads lock once per lock/unlock operation, and only a few operations are performed before the lock is released.  This should greatly reduce contention on the pthreads lock and make it practical to spin-wait using `pthreads_mutex_trylock()` to ensure that kernel calls are never required to acquire/release a lock.  However, the downside to esl_queuelock is that waiting threads are actively spin-waiting on their independent locations, making it hard for the scheduler to determine which thread to evict if there are more threads than cores.  Thus, care should be taken to ensure that we are not over-subscribing the processor.

The following table lists the functions in the `esl_queuelock` API.

| Function                       | Synopsis                                                     |
|--------------------------------|--------------------------------------------------------------|
| `esl_queuelock_Create()`       | Creates an esl_queuelock object                              |
| `esl_queuelock_Destroy()`      | Destroys an esl_queuelock object and frees its memory        |
| `esl_queuelock_Lock()`         | Acquires the lock, blocking until it does so                  |
| `esl_queuelock_Unlock()`       | Releases the lock, passing it to the next waiter if any      |

