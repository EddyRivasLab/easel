
This appendix describes Easel's coding style. It will probably only be
of interest to Easel developers, not to application developers who are
just using the Easel library.

\section{Test and example code}

Each module contains one or more \ccode{main()} functions for testing
and example purposes, enclosed in \ccode{\#ifdef}'s to control
conditional compilation.

There are currently five different types of \ccode{main()} functions
in Easel modules:

\begin{description} 

\item[\textbf{Examples}] Every module has at least one example
  \ccode{main()} that provides a ``hello world'' level example of
  using the module's API. Examples are enclosed in \ccode{cexcerpt}
  tags for extraction and verbatim inclusion in the documentation.
  They are enclosed by \ccode{\#ifdef eslFOO\_EXAMPLE} tags, where
  \ccode{FOO} is the name of the module.

\item[\textbf{Automated test drivers}] These are run by the automated
  test suite in \ccode{testsuite/testsuite.sqc} when one does a
  \ccode{make check} on the package. They include unit tests on
  individual functions as well as more sophisticated tests of the
  module as a whole. They are enclosed by \ccode{\#ifdef
  eslFOO\_TESTDRIVE} tags.

\item[\textbf{Regression/comparison tests}] These tests link to at
  least one other existing library that provides comparable
  functionality, such as the old SQUID library or the GNU Scientific
  Library, and test that Easel's functionality performs at least as
  well as the 'competition'. These tests are run on demand, and not
  included in automated testing, because the other libraries may only
  be present on a subset of our development machines. They are
  enclosed by \ccode{\#ifdef eslFOO\_REGRESSION} tags.

\item[\textbf{Benchmark tests}] These tests run a standardized
  performance benchmark and collect time and/or memory
  statistics. They may generate output suitable for graphing. They are
  run on demand. They are enclosed by \ccode{\#ifdef eslFOO\_BENCHMARK} tags.

\item[\textbf{Statistics generators}] These tests collect statistics
  used to characterize the module's scientific performance, such as
  its accuracy at some task. They may generate graphing output, and they
  are run on demand. They are enclosed by \ccode{\#ifdef eslFOO\_STATS} tags.
\end{description}  

All modules have at least one test driver and one example. Other tests
and examples are optional. When there is more than one \ccode{main()}
of a given type, the additional tags are numbered starting from 2: for
example, a module with three example \ccode{main()'s} would have three
tags for conditional compilation, \ccode{eslFOO\_EXAMPLE},
\ccode{eslFOO\_EXAMPLE2}, and \ccode{eslFOO\_EXAMPLE3}.


Which driver is compiled (if any) is controlled by conditional
compilation of the module \ccode{.c} file with the appropriate
flag. For example, to compile and run the \eslmod{sqio} test driver as
a standalone module:

\begin{cchunk}
   %  gcc -g -Wall -I. -o test -DeslSQIO_TESTDRIVE esl_sqio.c easel.c -lm
   %  ./test
\end{cchunk}

or to compile and run it in full library configuration:

\begin{cchunk}
   %  gcc -g -Wall -I. -L. -o test -DeslSQIO_TESTDRIVE esl_sqio.c -leasel -lm
   %  ./test
\end{cchunk}

Regressions, benchmarks, and stats tests may be full-fledged Easel
applications that link to the complete library and use any module.
The main automated test (the one compiled by defining \ccode{\#ifdef
eslFOO\_TESTDRIVE}) must be a minimal program, though, designed to be
linked to a minimal subset of Easel.This is especially important in
the Easel core modules, which need to be tested in standalone
configuration, one or more augmented configurations, and full library
configuration. Consequently, for example, the main automated test
never uses \ccode{getopts}, whereas other tests usually do. This makes
the main automated test rigid (which is ok, because it's automated)
and the other tests more configurable (which is ok, because they're
run on demand). 

\subsection{Automated test drivers}

A \esldef{test driver} is for unit testing at the module and
individual function levels. It provides a \ccode{main()} that runs
some (preferably all) of a module's API.

Each module shall have at least one test driver.

A test driver shall work in minimal standalone configuration, in any
possible augmented configuration, or in full library
configuration. Example compilations:
\begin{cchunk}
      % gcc -g -Wall -o test -I. -DeslMSA_TESTDRIVE esl_msa.c easel.c -lm
      % gcc -g -Wall -o test -I. -DeslMSA_TESTDRIVE -DeslAUGMENT_ALPHABET esl_msa.c esl_alphabet.c easel.c -lm
      % gcc -g -Wall -o test -I. -L. -DeslMSA_TESTDRIVE esl_msa.c -leasel -lm
\end{cchunk}

A test driver shall take no command line arguments, and shall depend
on no external files.  Any files that it might need, it generates as
tmpfiles for itself. 

A test driver shall return zero (\ccode{eslOK}) on success, and a
nonzero code on failure, as is standard for UNIX
applications. Automated test harnesses (including \ccode{sqc}) rely on
this.

A test driver generates no output upon success. Upon detecting a
failure, a test driver shall terminate execution as soon as possible
to facilitate tracing/debugging by calling \ccode{esl\_fatal()}, with
an informative error message. (The automated test harness,
\ccode{sqc}, will discard this message.) \footnote{Calling
\ccode{abort()} doesn't work because it generates an abnormal signal
that messes up \ccode{sqc}'s output. Calling \ccode{exit(1)} doesn't
leave an informative message, so a command line user can't readily
tell an error occurred.  Throwing an exception doesn't work because we
will also be testing the code under a nonfatal exception handler.}
Garbage collection upon a failure (including memory, opened files, or
created temporary files) is not required, and may even be
counterproductive.  For example, for debugging purposes, it may be
advantageous not to delete a temporary file from a failed test.

Thus, from the command line, a successful test run looks like:

\begin{cchunk}
      % ./test
      %
\end{cchunk}

and a failure looks like:

\begin{cchunk}
      % ./test
      yoyodyne exercise on fire, send help.
      %
\end{cchunk}

If a unit test or testdriver needs to create a temporary file (to test
i/o), the tmpfile is created with \ccode{esl\_tmpfile()}:

\begin{cchunk}
   char  tmpfile[] = "/tmp/eslXXXXXX";
   FILE *fp;

   if (esl_tmpfile(tmpfile, &fp) != eslOK) esl_fatal("failed to create tmpfile");
   write_stuff_to(fp);
   fclose(fp);

   if ((fp = fopen(tmpfile)) == NULL) esl_fatal("failed to open tmpfile");
   read_stuff_from(fp);
   fclose(fp);

   remove(tmpfile);
\end{cchunk}

Thus tmp files created by Easel's test suite have a common naming
convention and exist in a common place. On a test failure, the tmp
file remains, to assist debugging; on a test success, the tmp file is
removed.

\subsection{Test drivers aim for complete code coverage}

Each function, procedure, and macro in the exposed API shall be
tested. Often this will take the form of one unit test function per
API component. These unit tests are in a separate section of the
module's code. The test driver calls each unit test, and possibly more
than once with different arguments.  A unit test for
\ccode{esl\_foo\_Baz()} is named \ccode{static int utest\_Baz()}. A
unit test returns \ccode{eslOK} on success.  On detecting a failure, a
unit test should call \ccode{abort()} or \ccode{esl\_fatal()}
immediately to facilitate tracing/debugging, rather than returning a
failure code to the driver. In some cases, when it is judged to be
inefficient to write unit tests at such a low level, unit testing is
instead done with composite test functions that exercise more than one
API component.

The testdriver shall respond to a \ccode{eslTEST\_THROWING}
compile-time definition by registering a nonfatal error handler, as
in:
\begin{cchunk}
      #ifdef eslTEST_THROWING
        esl_error_SetHandler(&esl_nonfatal_handler);
      #endif
\end{cchunk}

Unit tests shall attempt to deliberately generate exceptions and
failures, and test that the appropriate error code is returned.  This
test code must be enclosed in \ccode{\#ifdef eslTEST\_THROWING} tags.
Exception testing cannot be done with the default fatal exception
handler installed, because exceptions would cause the program to
terminate with a nonzero code, and this would look like a test failure
to an automated test harness.

A test driver shall exercise all possible augmentations.

We currently use the GNU \textsc{gcov} program to measure code
coverage. \textsc{gcov} works best with unoptimized code, so that the
optimizer doesn't combine any lines of code, and it is only compatible
with the \textsc{gcc} compiler. An example of measuring code coverage
for the \eslmod{msa} module in full library configuration:

\begin{cchunk}
  % make distclean
  % ./configure --enable-debugging
  % make
  % gcc -fprofile-arcs -ftest-coverage -g -Wall -o test -L. -I. -DeslMSA_TESTDRIVE esl_msa.c -leasel -lm
  % ./test
  % gcov esl_msa.c
  File `esl_msa.c'
  Lines executed:65.30% of 1317
  esl_msa.c:creating `esl_msa.c.gcov'
\end{cchunk}

The file \ccode{esl\_msa.c.gcov} contains an annotated source listing
of the \ccode{.c} file, showing which lines were and weren't covered
by the test suite.



